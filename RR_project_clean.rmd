---
title: "Vehicle Loan Default Prediction"
author: "Agata Tomaszewska, Joanna Misiak, Ilya Kidyshka"
output:
  html_document: default
  pdf_document: default
  word_document: default
date: "2025-06-02"
editor_options: 
  markdown: 
    wrap: 72
---

## Project description

This project explores the prediction of loan default status using a
machine learning classification approach. The notebook includes data
preprocessing, exploratory data analysis, feature engineering, and the
application of classification algorithms such as Logistic Regression and
Random Forest. The goal is to accurately classify whether a loan
applicant is likely to default, leveraging real-world financial data and
evaluating model performance through metrics like accuracy and confusion
matrix. The project provides insights into important predictive features
and demonstrates a practical workflow for tackling binary classification
problems in the financial domain.

## Table of contents

1.  [Data loading]
2.  [Preliminary Data Analysis]
3.  [Explanatory Data Analysis]
  3.1. [Class distribution]
  3.2. [Default vs Disbursal date]
  3.3. [Univariate analysis]

## Data loading

```{r setup, message=FALSE}
options(repos = c(CRAN = "https://cloud.r-project.org"))
library(dplyr)
library(tidyr)
library(stringr)
library(formattable)
library(ggcorrplot)
suppressWarnings({
  library(ggplot2)
  library(lubridate)
  library(gridExtra)
})
library(ggcorrplot)
library(readr)
```

```{r}
train <- read_csv("train.csv")
test  <- read.csv("test.csv")
df    <- bind_rows(train, test)
```

## Preliminary Data Analysis

```{r}
cat("Shape of training dataframe: ", dim(train), "\n")
cat("Shape of testing dataframe: ", dim(test), "\n")

train <- train[!duplicated(train), ]
test <- test[!duplicated(test), ]

cat("Shape of training dataframe after removing duplicates: ", dim(train), "\n")
cat("Shape of testing dataframe after removing duplicates: ", dim(test), "\n")
```

```{r inspection Variable Inspection}
### Variable Inspection

cat("Names of columns: ", colnames(train), "\n")


```

We have 41 variables, no duplicates were detected. Overall, we have over
230 thousand observations in our training set. Let's check whether the
dataset has any missing values.

```{r}
total <- nrow(train)
missing_data <- train %>%
  summarise(across(everything(), ~sum(is.na(.)))) %>%
  pivot_longer(everything(), names_to = "column name", values_to = "Total missing") %>%
  mutate(`Percent missing` = (`Total missing` / total) * 100) %>%
  arrange(desc(`Total missing`))
print(missing_data)
```

We can see that more that 3% of the 'EMPLOYMENT_TYPE' variable's values
is missing. We will treat the NA values as not employed.

```{r}
print(unique(train$EMPLOYMENT_TYPE))
length(unique(train$EMPLOYMENT_TYPE))
```
Let's analyze the structure of the data set. We can see that most of the variables are numeric. We see that 'AVERAGE_ACCT_AGE' and 'CREDIT_HISTORY_LENGHT' can be changed from characters to numbers. 
```{r}
str(train)
```


```{r}
parse_age <- function(x) {
  years <- as.numeric(str_extract(x, "\\d+(?=yrs)"))
  months <- as.numeric(str_extract(x, "\\d+(?=mon)"))
  years[is.na(years)] <- 0
  months[is.na(months)] <- 0
  years + months / 12
}

train <- train %>%
  mutate(AVERAGE_ACCT_AGE = parse_age(AVERAGE_ACCT_AGE),
         CREDIT_HISTORY_LENGTH = parse_age(CREDIT_HISTORY_LENGTH))

test <- test %>%
  mutate(AVERAGE_ACCT_AGE = parse_age(AVERAGE_ACCT_AGE),
         CREDIT_HISTORY_LENGTH = parse_age(CREDIT_HISTORY_LENGTH))

```

Additionally, we changed the format of variables 'DATE_OF_BIRTH' and 'DISBURSAL_DATE' form object to date. 

```{r}

train$DATE_OF_BIRTH <- as.Date(train$DATE_OF_BIRTH, format = "%d-%m-%Y")
test$DATE_OF_BIRTH  <- as.Date(test$DATE_OF_BIRTH,  format = "%d-%m-%Y")


train$DISBURSAL_DATE <- as.Date(train$DISBURSAL_DATE, format = "%d-%m-%Y")
test$DISBURSAL_DATE  <- as.Date(test$DISBURSAL_DATE,  format = "%d-%m-%Y")
```

Finally, we obtained the dataset shaped like this:

```{r}
str(train)
```

## Explanatory Data Analysis
Now we can proceed with the EDA. Let's start with

### Class distribution
We will analyze the distribution of the target variable 'LOAN_DEFAULT'.

```{r}
class_df <- train %>%
  group_by(LOAN_DEFAULT) %>%
  summarise(UNIQUEID_count = n()) %>%
  arrange(desc(UNIQUEID_count))

formattable(class_df, list( UNIQUEID_count = color_bar("lightgreen") ))
```


```{r}
colors <- c("0" = "deepskyblue", "1" = "deeppink")

ggplot(train, aes(x = factor(LOAN_DEFAULT), fill = factor(LOAN_DEFAULT))) +
  geom_bar() +
  scale_fill_manual(values = colors) +
  labs(title = "Class Distribution", x = "Loan Default", y = "Count") +
  theme_minimal()

count_default_0 <- sum(train$LOAN_DEFAULT == 0, na.rm = TRUE)
count_default_1 <- sum(train$LOAN_DEFAULT == 1, na.rm = TRUE)
total <- count_default_0 + count_default_1

percentage_0 <- (count_default_0 / total) * 100
percentage_1 <- (count_default_1 / total) * 100

cat(sprintf("%% of no defaults       : %.2f%%\n", percentage_0))
cat(sprintf("Number of no defaults  : %d\n", count_default_0))
cat(sprintf("%% of defaults          : %.2f%%\n", percentage_1))
cat(sprintf("Number of defaults     : %d\n", count_default_1))
```
Also, let's analyze the distribution between other categorical variables, such as 'EMPLOYMENT_TYPE', 'MOBILENO_AVL_FLAG', 'AADHAR_FLAG', 'PAN_FLAG', 'VOTERID_FLAG', 'DRIVING_FLAG' and 'PASSPORT_FLAG'

```{r}
cat("Employment type\n\n")
train %>%
  group_by(EMPLOYMENT_TYPE, LOAN_DEFAULT) %>%
  summarise(count = n(), .groups = 'drop') %>%
  group_by(EMPLOYMENT_TYPE) %>%
  mutate(prop = count / sum(count)) %>%
  arrange(EMPLOYMENT_TYPE, LOAN_DEFAULT) %>%
  print(n = Inf)

cat("##############\n")

cat("Mobile Flag\n")
train %>%
  group_by(MOBILENO_AVL_FLAG, LOAN_DEFAULT) %>%
  summarise(count = n(), .groups = 'drop') %>%
  group_by(MOBILENO_AVL_FLAG) %>%
  mutate(percentage = count / sum(count)) %>%
  print()

# Aadhar Flag
cat("Aadhar Flag\n")
train %>%
  group_by(AADHAR_FLAG, LOAN_DEFAULT) %>%
  summarise(count = n(), .groups = 'drop') %>%
  group_by(AADHAR_FLAG) %>%
  mutate(percentage = count / sum(count)) %>%
  print()

cat("Pan Flag\n")
train %>%
  group_by(PAN_FLAG, LOAN_DEFAULT) %>%
  summarise(count = n(), .groups = 'drop') %>%
  group_by(PAN_FLAG) %>%
  mutate(percentage = count / sum(count)) %>%
  print()

# Voter ID Flag
cat("Voter ID Flag\n")
train %>%
  group_by(VOTERID_FLAG, LOAN_DEFAULT) %>%
  summarise(count = n(), .groups = 'drop') %>%
  group_by(VOTERID_FLAG) %>%
  mutate(percentage = count / sum(count)) %>%
  print()

cat("Driving L Flag\n")
train %>%
  group_by(DRIVING_FLAG, LOAN_DEFAULT) %>%
  summarise(count = n(), .groups = 'drop') %>%
  group_by(DRIVING_FLAG) %>%
  mutate(percentage = count / sum(count)) %>%
  print()

cat("Passport\n")
train %>%
  group_by(PASSPORT_FLAG, LOAN_DEFAULT) %>%
  summarise(count = n(), .groups = 'drop') %>%
  group_by(PASSPORT_FLAG) %>%
  mutate(percentage = count / sum(count)) %>%
  print()

train %>%
  group_by(LOAN_DEFAULT, EMPLOYMENT_TYPE, AADHAR_FLAG, PAN_FLAG, DRIVING_FLAG, PASSPORT_FLAG, VOTERID_FLAG) %>%
  summarise(count = n(), .groups = 'drop') %>%
  arrange(desc(count)) %>%
  print()

train_0 <- train %>% filter(LOAN_DEFAULT == 0)
train_1 <- train %>% filter(LOAN_DEFAULT == 1)

```
We can observe that the distribution between defaulted and non-defaulted client across all analyzed variables fluctuates around 21/79. 

### Default vs. Disbursal date

```{r}
train$DISBURSAL_DATE <- as.Date(train$DISBURSAL_DATE)

train$LOAN_DEFAULT <- as.factor(train$LOAN_DEFAULT)

ggplot(train, aes(x = DISBURSAL_DATE, fill = LOAN_DEFAULT)) +
  geom_histogram(data = subset(train, LOAN_DEFAULT == 1), 
                 bins = 50, alpha = 0.8, fill = "deeppink") +
  geom_histogram(data = subset(train, LOAN_DEFAULT == 0), 
                 bins = 50, alpha = 0.8, fill = "deepskyblue") +
  facet_wrap(~LOAN_DEFAULT, ncol = 1, scales = "free_y",
             labeller = as_labeller(c(`0` = "No default", `1` = "Default"))) +
  labs(x = "DISBURSAL DATE", y = "Number of Loans") +
  theme_bw() +
  theme(legend.position = "none")
```

### Univariate analysis
 First let's define a couple of useful functions that will help us quickly perform univariate analysis.
```{r}
# Plot distribution of one feature

plot_distribution <- function(feature, color = "steelblue") {
ggplot(train, aes_string(x = feature)) + geom_histogram(aes(y =
..density..), fill = color, bins = 100, alpha = 0.7, na.rm = TRUE) +
geom_density(color = "black", size = 1, na.rm = TRUE) + labs(title =
paste("Distribution of", feature), x = feature, y = "Density") +
theme_minimal() }
```

```{r}
# Plot distribution of multiple features, with TARGET = 1/0 on the same graph

plot_bar_comp <- function(var, nrow = 2) {
  plot_list <- list()
  
  for (feature in var) {
    p <- ggplot(train, aes_string(x = feature)) +
      geom_bar(fill = "skyblue") +
      labs(x = feature, y = "Count plot") +
      theme_minimal(base_size = 12)
    plot_list[[length(plot_list) + 1]] <- p
  }
  
  ncol <- 2
  do.call(grid.arrange, c(plot_list, nrow = nrow, ncol = ncol))
}

```

```{r}
# Box Plot for one feature

plot_box <- function(feature, color = "skyblue") { ggplot(train,
aes_string(y = feature)) + geom_boxplot(fill = color, outlier.color =
"red", na.rm = TRUE) + labs(title = paste("Box Plot of", feature), y =
feature) + theme_minimal() }
```

```{r}
# Bar Plot for one feature

plot_bar <- function(feature) { ggplot(train, aes_string(y = feature,
fill = "factor(LOAN_DEFAULT)")) + geom_bar(position = "dodge", color =
"black") + scale_fill_manual(values = c("0" = "skyblue", "1" = "pink"),
name = "LOAN_DEFAULT", labels = c("No Default", "Default")) + labs(title
= paste("Bar Plot of", feature, "by Loan Default"), y = feature, x =
"Count") + theme_minimal() + theme( axis.text.y = element_text(size =
10), plot.title = element_text(size = 14, face = "bold") ) }
```

Now, let's analyze the most important explanatory variable: **'DISBURSED_AMOUNT'**.

```{r}
summary(train$DISBURSED_AMOUNT)

plot_distribution("DISBURSED_AMOUNT", "green")

plot_box("DISBURSED_AMOUNT", "green")

```
Based on the box plot, we can observe the presence of the outliers. We will try two methods of dealing with outliers: replacing the outliers' values with a sample mean and binning. 
But first, let's define functions that will speed up the outlier detection process. We will create a function that will calculate all necessary statistical values: mean, lower and upper threshold.

```{r}
outlier_data <- function(df, feature) {
  # Number of observations
  obs <- length(df[[feature]])
  cat("No. of observations in column:", obs, "\n")
  
  # Descriptive statistics
  data_mean <- mean(df[[feature]], na.rm = TRUE)
  data_sd   <- sd(df[[feature]], na.rm = TRUE)
  cat(sprintf("Statistics: Mean = %.3f, Std dev = %.3f\n", data_mean, data_sd))
  
  # Thresholds for outliers, set as 3 standard deviations
  cut_off <- data_sd * 3
  lower <- data_mean - cut_off
  upper <- data_mean + cut_off
  
  # Outliers count
  outliers <- df[[feature]][df[[feature]] < lower | df[[feature]] > upper]
  cat("Identified outliers:", length(outliers), "\n")
  
  return(list(lower = lower, upper = upper, mean = data_mean))
}
```
Now, let's play with smoothing the outliers. We will create the function that will impute the outstanding obseravtions. 
```{r}
impute_outlier <- function(vec, lower, upper, mean_val) {
  sapply(vec, function(x) {
    if (is.na(x)) {
      return(NA)
    } else if (x <= lower || x >= upper) {
      return(mean_val)
    } else {
      return(x)
    }
  })
}
```

```{r}
disbursed_amount_stats <- outlier_data(train, 'DISBURSED_AMOUNT')
```

We manage to identify more that 3000 outliers. Now we will replace the values with the mean. 

```{r}
train$DISBURSED_AMOUNT_new <- impute_outlier(train$DISBURSED_AMOUNT, disbursed_amount_stats$lower, disbursed_amount_stats$upper, disbursed_amount_stats$mean)

# No. of observations after the imputation

cat("No. of observations in column: ", length(train$DISBURSED_AMOUNT_new), "\n")
```
Now let's try binning. Firstly, we have too divide our variable's range into bins. We chose to divide it to 4 bins based on quantiles.
```{r}
bin_labels <- c("Low", "Medium", "High", "Extreme")

quantiles <- quantile(train$DISBURSED_AMOUNT, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE)

# Quantile distribution
train$DISBURSED_AMOUNT_bins <- cut(train$DISBURSED_AMOUNT,
                                   breaks = quantiles,
                                   include.lowest = TRUE,
                                   labels = bin_labels)

table(train$DISBURSED_AMOUNT_bins)

plot_bar("DISBURSED_AMOUNT_bins")
```
Now let's analyze another variable: **'ASSET_COST'**
```{r}
summary(train$ASSET_COST)
plot_distribution <- function(feature, color = "steelblue") {
  ggplot(train, aes_string(x = feature)) +
    geom_histogram(aes(y = ..density..), fill = color, bins = 100, alpha = 0.7, na.rm = TRUE) +
    geom_density(color = "black", size = 1, na.rm = TRUE) +
    labs(title = paste("Distribution of", feature), x = feature, y = "Density") +
    theme_minimal()
}
plot_distribution("ASSET_COST", "tomato")
plot_box("ASSET_COST", "tomato")
```
Again, we can notice the presence of the outliers. We can apply similar approach as before.

Imputation
```{r}
asset_cost_stats <- outlier_data(train, 'ASSET_COST')
```
Almost 4500 outliers detected - let's try smoothing them out with mean
```{r}
train$ASSET_COST_new <- impute_outlier(train$ASSET_COST, asset_cost_stats$lower, asset_cost_stats$upper, asset_cost_stats$mean)

# No. of observations after the imputation

cat("No. of observations in column: ", length(train$ASSET_COST_new), "\n")
```
Binning
```{r}
quantiles <- quantile(train$ASSET_COST, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE)

#Quantile distribution
train$ASSET_COST_bins <- cut(train$ASSET_COST,
                                   breaks = quantiles,
                                   include.lowest = TRUE,
                                   labels = bin_labels)

table(train$ASSET_COST_bins)

plot_bar("ASSET_COST_bins")
```
**'LTV'**
```{r}
summary(train$LTV)
plot_distribution <- function(feature, color = "steelblue") {
  ggplot(train, aes_string(x = feature)) +
    geom_histogram(aes(y = ..density..), fill = color, bins = 100, alpha = 0.7, na.rm = TRUE) +
    geom_density(color = "black", size = 1, na.rm = TRUE) +
    labs(title = paste("Distribution of", feature), x = feature, y = "Density") +
    theme_minimal()
}
plot_distribution("LTV", "tomato")
plot_box("LTV", "tomato")
```
Imputation
```{r}
LTV_stats <- outlier_data(train, 'LTV')
```

```{r}
train$LTV_new <- impute_outlier(train$LTV, LTV_stats$lower, LTV_stats$upper, LTV_stats$mean)

# No. of observations after the imputation

cat("No. of observations in column: ", length(train$LTV_new), "\n")
```
Binning
```{r}
quantiles <- quantile(train$LTV, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE)

#Quantile distribution
train$LTV_bins <- cut(train$LTV,
                                   breaks = quantiles,
                                   include.lowest = TRUE,
                                   labels = bin_labels)

table(train$LTV_bins)

plot_bar("LTV_bins")
```
**'PERFORM_CNS_SCORE'**
```{r}
summary(train$PERFORM_CNS_SCORE)
plot_distribution <- function(feature, color = "steelblue") {
  ggplot(train, aes_string(x = feature)) +
    geom_histogram(aes(y = ..density..), fill = color, bins = 100, alpha = 0.7, na.rm = TRUE) +
    geom_density(color = "black", size = 1, na.rm = TRUE) +
    labs(title = paste("Distribution of", feature), x = feature, y = "Density") +
    theme_minimal()
}
plot_distribution("PERFORM_CNS_SCORE", "tomato")
plot_box("PERFORM_CNS_SCORE", "tomato")
```
Imputation
```{r}
perform_cns_score_stats <- outlier_data(train, 'PERFORM_CNS_SCORE')
```

```{r}
train$PERFORM_CNS_SCORE_new <- impute_outlier(train$PERFORM_CNS_SCORE, perform_cns_score_stats$lower, perform_cns_score_stats$upper, perform_cns_score_stats$mean)

# No. of observations after the imputation

cat("No. of observations in column: ", length(train$PERFORM_CNS_SCORE_new), "\n")
```
Binning
```{r}
bin_labels = c("No History",'Very Low', "Low" ,'Medium', 'High')
cut_bins = c(-1,10,150, 350, 650, 1000)

quantiles <- quantile(train$PERFORM_CNS_SCORE, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE)

#Quantile distribution
train$PERFORM_CNS_SCORE_bins <- cut(train$PERFORM_CNS_SCORE,
                                   breaks = cut_bins,
                                   include.lowest = TRUE,
                                   labels = bin_labels)

table(train$PERFORM_CNS_SCORE_bins)

plot_bar("PERFORM_CNS_SCORE_bins")
```
**'PERFORM_CNS_SCORE_DESCRIPTION'**
```{r}
train %>%
  group_by(PERFORM_CNS_SCORE_DESCRIPTION, PERFORM_CNS_SCORE_bins) %>%
  summarise(count = n(), .groups = "drop") %>%
  arrange(desc(count))

table(train$PERFORM_CNS_SCORE_DESCRIPTION)

gg <- train %>%
  group_by(PERFORM_CNS_SCORE_DESCRIPTION, LOAN_DEFAULT) %>%
  summarise(counts = n(), .groups = "drop") %>%
  group_by(PERFORM_CNS_SCORE_DESCRIPTION) %>%
  mutate(percentage = counts / sum(counts) * 100) %>%
  ungroup()

print(gg)
```
**'PRI_NO_OF_ACCTS'**
```{r}
summary(train$PRI_NO_OF_ACCTS)
plot_distribution <- function(feature, color = "steelblue") {
  ggplot(train, aes_string(x = feature)) +
    geom_histogram(aes(y = ..density..), fill = color, bins = 100, alpha = 0.7, na.rm = TRUE) +
    geom_density(color = "black", size = 1, na.rm = TRUE) +
    labs(title = paste("Distribution of", feature), x = feature, y = "Density") +
    theme_minimal()
}
plot_distribution("PRI_NO_OF_ACCTS", "tomato")
plot_box("PRI_NO_OF_ACCTS", "tomato")
```
Imputation
```{r}
pri_no_of_accts_stats <- outlier_data(train, 'PRI_NO_OF_ACCTS')
```

```{r}
train$PRI_NO_OF_ACCTS_new <- impute_outlier(train$PRI_NO_OF_ACCTS, pri_no_of_accts_stats$lower, pri_no_of_accts_stats$upper, pri_no_of_accts_stats$mean)

# No. of observations after the imputation

cat("No. of observations in column: ", length(train$PRI_NO_OF_ACCTS_new), "\n")
```
Binning
```{r}
bin_labels <- c("One", "More than One")
cut_bins <- c(-1, 1, 1000)

train$PRI_NO_OF_ACCTS_bins <- cut(train$PRI_NO_OF_ACCTS,
                                   breaks = cut_bins,
                                   include.lowest = TRUE,
                                   labels = bin_labels)

table(train$PRI_NO_OF_ACCTS_bins)

plot_bar("PRI_NO_OF_ACCTS_bins")
```
**'PRI_OVERDUE_ACCTS'**
```{r}
summary(train$PRI_OVERDUE_ACCTS)
plot_distribution <- function(feature, color = "steelblue") {
  ggplot(train, aes_string(x = feature)) +
    geom_histogram(aes(y = ..density..), fill = color, bins = 100, alpha = 0.7, na.rm = TRUE) +
    geom_density(color = "black", size = 1, na.rm = TRUE) +
    labs(title = paste("Distribution of", feature), x = feature, y = "Density") +
    theme_minimal()
}
plot_distribution("PRI_OVERDUE_ACCTS", "tomato")
plot_box("PRI_OVERDUE_ACCTS", "tomato")
```
Imputation
```{r}
pri_overdue_accts_stats <- outlier_data(train, 'PRI_OVERDUE_ACCTS')
```

```{r}
train$PRI_OVERDUE_ACCTS_new <- impute_outlier(train$PRI_OVERDUE_ACCTS, pri_overdue_accts_stats$lower, pri_overdue_accts_stats$upper, pri_overdue_accts_stats$mean)

# No. of observations after the imputation

cat("No. of observations in column: ", length(train$PRI_OVERDUE_ACCTS_new), "\n")
```
Binning
```{r}
bin_labels <- c("None", "One (or more)")
cut_bins <- c(-1, 0, 1000)

train$PRI_OVERDUE_ACCTS_bins <- cut(train$PRI_OVERDUE_ACCTS,
                                   breaks = cut_bins,
                                   include.lowest = TRUE,
                                   labels = bin_labels)

table(train$PPRI_OVERDUE_ACCTS_bins)

plot_bar("PRI_OVERDUE_ACCTS_bins")
```

Let's look into data with lesser importance

```{r}
var <- c("MOBILENO_AVL_FLAG", "AADHAR_FLAG", "PAN_FLAG", "VOTERID_FLAG", "PASSPORT_FLAG", "DRIVING_FLAG")
plot_bar_comp(var, nrow = 3)
```

# Employment Type

ggplot(train, aes(x = EMPLOYMENT_TYPE, fill = factor(LOAN_DEFAULT))) +
geom_bar(position = "dodge", color = "black") + labs(title =
"EMPLOYMENT_TYPE vs LOAN_DEFAULT", x = "Employment Type", y = "Count",
fill = "Loan Default") + theme_minimal() + scale_fill_manual(values =
c("0" = "skyblue", "1" = "tomato"))

install.packages('lubridate') library(lubridate)

# Age is in days

train \<- train %\>% mutate(age = as.numeric(Sys.Date() -
DATE_OF_BIRTH))

train$age <- as.integer(train$age) head(train\$age)

str(train\$DISBURSAL_DATE)

train \<- train %\>% mutate(disbursal_time =
as.integer(difftime(Sys.Date(), DISBURSAL_DATE, units = "days")))

head(train\$disbursal_time)

# MANUFACTURER_ID

ggplot(train, aes(x = factor(MANUFACTURER_ID), fill =
factor(LOAN_DEFAULT))) + geom_bar(position = "dodge", color = "black") +
labs(title = "MANUFACTURER_ID vs LOAN_DEFAULT", x = "MANUFACTURER_ID", y
= "Count", fill = "Loan Default") + scale_fill_manual(values = c("0" =
"skyblue", "1" = "tomato"), labels = c("No Default", "Default")) +
theme_minimal() + theme(axis.text.x = element_text(angle = 45, hjust =
1))

ggplot(train, aes(x = factor(BRANCH_ID), fill = factor(LOAN_DEFAULT))) +
geom_bar(position = "dodge", color = "black") + labs(title = "BRANCH_ID
vs LOAN_DEFAULT", x = "BRANCH_ID", y = "Count", fill = "Loan Default") +
scale_fill_manual(values = c("0" = "skyblue", "1" = "tomato"), labels =
c("No Default", "Default")) + theme_minimal() + theme(axis.text.x =
element_text(angle = 45, hjust = 1))

plot_distribution_comp \<- function(features, nrow = 2) { plots \<-
list()

for (feature in features) { p \<- ggplot(train, aes_string(x = feature,
fill = "factor(LOAN_DEFAULT)", color = "factor(LOAN_DEFAULT)")) +
geom_histogram(aes(y = ..density..), bins = 100, position = "identity",
alpha = 0.3, na.rm = TRUE) + geom_density(size = 1.2, na.rm = TRUE) +
labs(title = paste("Distribution of", feature), x = feature, y =
"Density", fill = "LOAN_DEFAULT", color = "LOAN_DEFAULT") +
scale_fill_manual(values = c("0" = "skyblue", "1" = "tomato")) +
scale_color_manual(values = c("0" = "skyblue", "1" = "tomato")) +
theme_minimal()

```         
plots[[length(plots) + 1]] <- p
```

}

do.call(grid.arrange, c(plots, nrow = nrow)) } \# Let's see the new
columns along with the less important continous variables

var \<- c("PRI_NO_OF_ACCTS_new", "PRI_ACTIVE_ACCTS",
"PRI_OVERDUE_ACCTS_new", "PRI_CURRENT_BALANCE", "PRI_SANCTIONED_AMOUNT",
"PRI_DISBURSED_AMOUNT")

plot_distribution_comp(var, nrow = 3)

var \<- c("SEC_NO_OF_ACCTS", "SEC_ACTIVE_ACCTS", "SEC_OVERDUE_ACCTS",
"SEC_CURRENT_BALANCE", "SEC_SANCTIONED_AMOUNT", "SEC_DISBURSED_AMOUNT")

plot_distribution_comp(var, nrow = 3)

## Feature Selection

train \<- train %\>% select(-DATE_OF_BIRTH, -STATE_ID,
-EMPLOYEE_CODE_ID, -SUPPLIER_ID, -MANUFACTURER_ID, -CURRENT_PINCODE_ID,
-BRANCH_ID)

library(ggcorrplot)

corr_cols \<- c("PRI_ACTIVE_ACCTS", "PRI_CURRENT_BALANCE",
"PRI_SANCTIONED_AMOUNT", "PRI_DISBURSED_AMOUNT", "SEC_NO_OF_ACCTS",
"SEC_ACTIVE_ACCTS", "SEC_OVERDUE_ACCTS", "SEC_CURRENT_BALANCE",
"SEC_SANCTIONED_AMOUNT", "SEC_DISBURSED_AMOUNT", "PRI_NO_OF_ACCTS_new",
"PRI_OVERDUE_ACCTS_new")

corr_data \<- train[, corr_cols]

corr_matrix \<- cor(corr_data, use = "pairwise.complete.obs")

#Highly Correlated

ggcorrplot(corr_matrix, hc.order = TRUE, type = "lower", lab = TRUE,
lab_size = 3, method = "square", colors = c("steelblue", "white",
"darkgreen"), title = "Macierz korelacji", ggtheme = theme_minimal())

#Not highly correlated with anyone: 'PRI_ACTIVE_ACCTS',
'PRI_CURRENT_BALANCE','PRI_SANCTIONED_AMOUNT',
\#'PRI_DISBURSED_AMOUNT','SEC_OVERDUE_ACCTS'¶ \#'PRI_NO_OF_ACCTS_new',
'PRI_OVERDUE_ACCTS_new'are perfectly positively correlated and hence
keeping one \#'SEC_NO_OF_ACCTS', 'SEC_ACTIVE_ACCTS' are highly
positively correlated, hence keeping one \#'SEC_CURRENT_BALANCE',
'SEC_SANCTIONED_AMOUNT', 'SEC_DISBURSED_AMOUNT' are highly positively
correlated, hence keeping one

corr_data \<- train[, c('SEC_INSTAL_AMT',
'PERFORM_CNS_SCORE','NEW_ACCTS_IN_LAST_SIX_MONTHS',
'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'AVERAGE_ACCT_AGE',
'CREDIT_HISTORY_LENGTH', 'NO_OF_INQUIRIES','age', 'disbursal_time')]

corr_matrix \<- cor(corr_data, use = "pairwise.complete.obs")

ggcorrplot(corr_matrix, hc.order = TRUE,\
type = "lower",\
lab = TRUE,\
lab_size = 3,\
method = "square",\
colors = c("steelblue", "white", "darkgreen"),\
title = "Macierz korelacji", ggtheme = theme_minimal())

\#'AVERAGE_ACCT_AGE', 'CREDIT_HISTORY_LENGTH'are highly positively
correlated and hence keeping one¶

corr_data \<- train[, c('PRI_ACTIVE_ACCTS', 'PRI_CURRENT_BALANCE',
'PRI_SANCTIONED_AMOUNT', 'PERFORM_CNS_SCORE', 'PRI_DISBURSED_AMOUNT',
'SEC_NO_OF_ACCTS', 'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE',
'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT', 'NEW_ACCTS_IN_LAST_SIX_MONTHS',
'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'CREDIT_HISTORY_LENGTH',
'NO_OF_INQUIRIES', 'DISBURSED_AMOUNT_new', 'ASSET_COST_new', 'LTV_new',
'PRI_NO_OF_ACCTS_new', 'age', 'disbursal_time')]

corr_matrix \<- cor(corr_data, use = "pairwise.complete.obs")

ggcorrplot(corr_matrix, hc.order = TRUE,\
type = "lower",\
lab = TRUE,\
lab_size = 1,\
method = "square",\
colors = c("steelblue", "white", "darkgreen"), title = "Macierz
korelacji", ggtheme = theme_minimal())

#One out of 'PRI_SANCTIONED_AMOUNT', 'PRI_DISBURSED_AMOUNT' #One out of
'LTV_new', 'PRI_NO_OF_ACCTS_new' #Eliminate
'NEW_ACCTS_IN_LAST_SIX_MONTHS

#Preparing Datasets 1) Binned Variables 2) Continous variables

train_con \<- train[, c('EMPLOYMENT_TYPE', 'MOBILENO_AVL_FLAG',
'AADHAR_FLAG', 'PAN_FLAG', 'VOTERID_FLAG', 'DRIVING_FLAG',
'PASSPORT_FLAG', 'PERFORM_CNS_SCORE', 'PERFORM_CNS_SCORE_DESCRIPTION',
'PRI_ACTIVE_ACCTS', 'PRI_CURRENT_BALANCE', 'PRI_DISBURSED_AMOUNT',
'SEC_NO_OF_ACCTS', 'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE',
'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT',
'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'CREDIT_HISTORY_LENGTH',
'NO_OF_INQUIRIES', 'LOAN_DEFAULT', 'DISBURSED_AMOUNT_new',
'ASSET_COST_new', 'LTV_new', 'age', 'disbursal_time')]

train_bin \<- train[, c('UNIQUEID', 'EMPLOYMENT_TYPE',
'MOBILENO_AVL_FLAG', 'AADHAR_FLAG', 'PAN_FLAG', 'VOTERID_FLAG',
'DRIVING_FLAG', 'PASSPORT_FLAG', 'PERFORM_CNS_SCORE',
'PERFORM_CNS_SCORE_DESCRIPTION', 'PRI_ACTIVE_ACCTS',
'PRI_CURRENT_BALANCE', 'PRI_DISBURSED_AMOUNT', 'SEC_NO_OF_ACCTS',
'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE', 'PRIMARY_INSTAL_AMT',
'SEC_INSTAL_AMT', 'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS',
'CREDIT_HISTORY_LENGTH', 'NO_OF_INQUIRIES', 'LOAN_DEFAULT',
'DISBURSED_AMOUNT_bins', 'ASSET_COST_bins', 'LTV_bins',
'PERFORM_CNS_SCORE_bins', 'PRI_NO_OF_ACCTS_bins',
'PRI_OVERDUE_ACCTS_bins', 'age', 'disbursal_time')]

# Wymagane biblioteki

library(ggplot2) library(reshape2)

# Confusion Matrix

plot_confusion_matrix \<- function(cm, classes, normalize = FALSE, title
= "Confusion Matrix") {

cm_df \<- as.data.frame(cm) colnames(cm_df) \<- c("True", "Predicted",
"Freq")

if (normalize) { cm_df \<- cm_df %\>% group_by(True) %\>% mutate(Freq =
Freq / sum(Freq)) %\>% ungroup() }

max_val \<- max(cm_df$Freq) cm_df$text_color \<- ifelse(cm_df\$Freq \>
max_val / 2, "white", "black")

ggplot(cm_df, aes(x = Predicted, y = True, fill = Freq)) +
geom_tile(color = "white") + geom_text(aes(label = round(Freq, if
(normalize) 2 else 0), color = text_color), size = 4) +
scale_fill_gradient(low = "white", high = "blue") +
scale_color_identity() + labs(title = title, x = "Predicted label", y =
"True label") + theme_minimal() }

# Precision, Recall, F1 Score

show_metrics \<- function(cm) { cm \<- as.matrix(cm)

tp \<- cm[1, 1] fn \<- cm[1, 0] fp \<- cm[0, 1] tn \<- cm[0, 0]

precision \<- tp / (tp + fp) recall \<- tp / (tp + fn) f1_score \<- 2 \*
((precision \* recall) / (precision + recall))

cat(sprintf("Precision = %.3f\n", precision)) cat(sprintf("Recall =
%.3f\n", recall)) cat(sprintf("F1_score = %.3f\n", f1_score)) }

# Precision-recall curve

plot_precision_recall \<- function(recall, precision) { df \<-
data.frame(recall = recall, precision = precision)

ggplot(df, aes(x = recall, y = precision)) + geom_step(direction = "hv",
alpha = 0.2, color = "blue") + geom_area(stat = "step", alpha = 0.2,
fill = "blue") + geom_line(size = 1.2, color = "blue") + xlim(0.0,
1.0) + ylim(0.0, 1.05) + labs(title = "Precision Recall Curve", x =
"Recall", y = "Precision") + theme_minimal() }

# ROC curve

plot_roc \<- function(fpr, tpr) { df \<- data.frame(fpr = fpr, tpr =
tpr)

ggplot(df, aes(x = fpr, y = tpr)) + geom_line(color = "blue", size =
1.2) + geom_abline(slope = 1, intercept = 0, linetype = "dashed", color
= "black", size = 1) + xlim(0.0, 0.001) + ylim(0.0, 1.05) + labs(title =
"ROC Curve", x = "False Positive Rate", y = "True Positive Rate") +
theme_minimal() }

#feature importance plot

plot_feature_importance \<- function(model, predictors) { tmp \<-
data.frame( Feature = predictors, Feature_importance = model\$importance
)

tmp \<- tmp[order(tmp\$Feature_importance, decreasing = TRUE), ]

ggplot(tmp, aes(x = reorder(Feature, Feature_importance), y =
Feature_importance)) + geom_bar(stat = "identity", fill = "steelblue") +
coord_flip() + labs(title = "Features importance", x = "Feature", y =
"Feature importance") + theme_minimal() + theme(plot.title =
element_text(size = 14)) }

## Standardization of data

scaleColumns \<- function(df, cols_to_scale) { for (col in
cols_to_scale) { df[[col]] \<- scale(df[[col]]) } return(df) }

scaled_df \<- scaleColumns(train_con, c('PERFORM_CNS_SCORE',
'PRI_ACTIVE_ACCTS', 'PRI_CURRENT_BALANCE', 'PRI_DISBURSED_AMOUNT',
'SEC_NO_OF_ACCTS', 'SEC_OVERDUE_ACCTS', 'SEC_CURRENT_BALANCE',
'PRIMARY_INSTAL_AMT', 'SEC_INSTAL_AMT',
'DELINQUENT_ACCTS_IN_LAST_SIX_MONTHS', 'CREDIT_HISTORY_LENGTH',
'NO_OF_INQUIRIES', 'DISBURSED_AMOUNT_new', 'ASSET_COST_new', 'LTV_new',
'age', 'disbursal_time'))

head(scaled_df)

library(fastDummies)

train_dummy \<- fastDummies::dummy_cols( scaled_df, remove_first_dummy =
TRUE,\
remove_selected_columns = TRUE,\
ignore_na = TRUE\
)

head(train_dummy)

library(caret) y \<- train_dummy[["LOAN_DEFAULT"]]

X \<- train_dummy[, setdiff(names(train_dummy), "LOAN_DEFAULT")]

dim(X)

set.seed(101) train_index \<- createDataPartition(y, p = 0.8, list =
FALSE) X_train \<- X[train_index, ] X_test \<- X[-train_index, ] y_train
\<- y[train_index] y_test \<- y[-train_index]

# Konfiguracja 10-krotnej walidacji krzyżowej z losowaniem

k_fold \<- trainControl(method = "cv", number = 10)
